{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfe7502",
   "metadata": {},
   "source": [
    "## Intent classifier trainer\n",
    "\n",
    "* Dataset: student_queries.csv file with synthetic messages, each labeled with one of the intent categories, such as: Course Information, Enrollment / Course Registration, Withdrawal or Drop Course, Scholarship/Financial Aid, etc.\n",
    "* Task: Single-label classification to predict the intent behind the student's query.\n",
    "* Model: Use BertForSequenceClassification, fine-tuned on the intent dataset.\n",
    "* Goal: To determine the purpose of a query.\n",
    "* The trained model is saved to models/intentClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5174a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_labels = [\n",
    "        \"Course Information\",\n",
    "        \"Enrollment / Course Registration\",\n",
    "        \"Withdrawal or Drop Course\",\n",
    "        \"Access Issues (portal/login)\",\n",
    "        \"Technical Support\",\n",
    "        \"Tuition/Fees Inquiry\",\n",
    "        \"Scholarship/Financial Aid\",\n",
    "        \"Mental Health Concerns\",\n",
    "        \"Stress or Burnout\",\n",
    "        \"Bullying or Harassment\",\n",
    "        \"Administrative Support\",\n",
    "        \"Campus Facilities\",\n",
    "        \"Housing/Accommodation\",\n",
    "        \"Extracurricular Activities\",\n",
    "        \"General Complaint\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_to_id = {label: idx for idx, label in enumerate(intent_labels)}\n",
    "id_to_label = {idx: label for idx, label in enumerate(intent_labels)}\n",
    "\n",
    "df = pd.read_csv(\"../../data/student_queries.csv\")\n",
    "\n",
    "# Encode labels\n",
    "df[\"label\"] = df[\"intent\"].map(label_to_id)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "# convert label to int (from float due to NaN)\n",
    "df[\"label\"] = df[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32bd868b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>student</th>\n",
       "      <th>question</th>\n",
       "      <th>intent</th>\n",
       "      <th>is_distressed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-04 14:07:23</td>\n",
       "      <td>Shannon Austin</td>\n",
       "      <td>What topics will be covered in the AI course?</td>\n",
       "      <td>Course Information</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-04 16:13:28</td>\n",
       "      <td>Stephanie Calhoun</td>\n",
       "      <td>I'm feeling really overwhelmed lately.</td>\n",
       "      <td>Mental Health Concerns</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-04 16:19:28</td>\n",
       "      <td>Kevin Garcia</td>\n",
       "      <td>Who do I contact for transcript requests?</td>\n",
       "      <td>Administrative Support</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-04 16:55:20</td>\n",
       "      <td>Lisa Duran</td>\n",
       "      <td>I have a complaint about the cafeteria service.</td>\n",
       "      <td>General Complaint</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-04 17:08:35</td>\n",
       "      <td>Jeff Rangel</td>\n",
       "      <td>Who do I contact for transcript requests?</td>\n",
       "      <td>Administrative Support</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime            student  \\\n",
       "0  2025-07-04 14:07:23     Shannon Austin   \n",
       "1  2025-07-04 16:13:28  Stephanie Calhoun   \n",
       "2  2025-07-04 16:19:28       Kevin Garcia   \n",
       "3  2025-07-04 16:55:20         Lisa Duran   \n",
       "4  2025-07-04 17:08:35        Jeff Rangel   \n",
       "\n",
       "                                          question                  intent  \\\n",
       "0    What topics will be covered in the AI course?      Course Information   \n",
       "1           I'm feeling really overwhelmed lately.  Mental Health Concerns   \n",
       "2        Who do I contact for transcript requests?  Administrative Support   \n",
       "3  I have a complaint about the cafeteria service.       General Complaint   \n",
       "4        Who do I contact for transcript requests?  Administrative Support   \n",
       "\n",
       "  is_distressed  label  \n",
       "0         False      0  \n",
       "1          True      7  \n",
       "2         False     10  \n",
       "3          True     14  \n",
       "4         False     10  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34cc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"question\", \"label\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"question\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57abb7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bennyxiong/Documents/Source/ML/CSCN8010_FinalProject/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading file vocab.txt from cache at /Users/bennyxiong/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/bennyxiong/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/bennyxiong/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2460/2460 [00:00<00:00, 10087.72 examples/s]\n",
      "Map: 100%|██████████| 615/615 [00:00<00:00, 9727.64 examples/s]\n",
      "loading configuration file config.json from cache at /Users/bennyxiong/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/bennyxiong/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"question\"], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Model setup\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(intent_labels)\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7df92ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, question. If __index_level_0__, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/bennyxiong/Documents/Source/ML/CSCN8010_FinalProject/.venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2460\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1540\n",
      "  Number of trainable parameters = 109493775\n",
      "/Users/bennyxiong/Documents/Source/ML/CSCN8010_FinalProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                        \n",
      "  0%|          | 0/1540 [04:21<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8348, 'learning_rate': 1.8701298701298704e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, question. If __index_level_0__, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 615\n",
      "  Batch size = 64\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "  0%|          | 0/1540 [04:42<?, ?it/s]          \n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./results/checkpoint-154\n",
      "Configuration saved in ./results/checkpoint-154/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2902831733226776, 'eval_accuracy': 0.9886178861788618, 'eval_f1': 0.9885946487373333, 'eval_precision': 0.9888852875239715, 'eval_recall': 0.9886178861788618, 'eval_runtime': 2.5486, 'eval_samples_per_second': 241.313, 'eval_steps_per_second': 3.924, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-154/pytorch_model.bin\n",
      "/Users/bennyxiong/Documents/Source/ML/CSCN8010_FinalProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                        \n",
      "  0%|          | 0/1540 [04:58<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4329, 'learning_rate': 1.7402597402597403e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/1540 [05:32<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0791, 'learning_rate': 1.6103896103896105e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, question. If __index_level_0__, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 615\n",
      "  Batch size = 64\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\u001b[A                                               \n",
      "\n",
      "  0%|          | 0/1540 [05:37<?, ?it/s]       \n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./results/checkpoint-308\n",
      "Configuration saved in ./results/checkpoint-308/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05840090662240982, 'eval_accuracy': 0.991869918699187, 'eval_f1': 0.9918486169913014, 'eval_precision': 0.99198606271777, 'eval_recall': 0.991869918699187, 'eval_runtime': 2.5189, 'eval_samples_per_second': 244.15, 'eval_steps_per_second': 3.97, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-308/pytorch_model.bin\n",
      "/Users/bennyxiong/Documents/Source/ML/CSCN8010_FinalProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                        \n",
      "  0%|          | 0/1540 [06:09<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0322, 'learning_rate': 1.4805194805194807e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, question. If __index_level_0__, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 615\n",
      "  Batch size = 64\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "  0%|          | 0/1540 [06:33<?, ?it/s]          \n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./results/checkpoint-462\n",
      "Configuration saved in ./results/checkpoint-462/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04369879141449928, 'eval_accuracy': 0.9902439024390244, 'eval_f1': 0.9902228425898717, 'eval_precision': 0.9903213317847464, 'eval_recall': 0.9902439024390244, 'eval_runtime': 2.5755, 'eval_samples_per_second': 238.793, 'eval_steps_per_second': 3.883, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-462/pytorch_model.bin\n",
      "/Users/bennyxiong/Documents/Source/ML/CSCN8010_FinalProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                        \n",
      "  0%|          | 0/1540 [06:47<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0196, 'learning_rate': 1.3506493506493508e-05, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/1540 [07:21<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0129, 'learning_rate': 1.2207792207792208e-05, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, question. If __index_level_0__, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 615\n",
      "  Batch size = 64\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\u001b[A                                               \n",
      "\n",
      "  0%|          | 0/1540 [07:29<?, ?it/s]       \n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./results/checkpoint-616\n",
      "Configuration saved in ./results/checkpoint-616/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0399785116314888, 'eval_accuracy': 0.9902439024390244, 'eval_f1': 0.9902228425898717, 'eval_precision': 0.9903213317847464, 'eval_recall': 0.9902439024390244, 'eval_runtime': 2.555, 'eval_samples_per_second': 240.707, 'eval_steps_per_second': 3.914, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-616/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-308 (score: 0.9918486169913014).\n",
      "                                        \n",
      " 40%|████      | 616/1540 [03:43<05:35,  2.76it/s]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, question. If __index_level_0__, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 615\n",
      "  Batch size = 64\n",
      "/Users/bennyxiong/Documents/Source/ML/CSCN8010_FinalProject/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 223.4306, 'train_samples_per_second': 110.101, 'train_steps_per_second': 6.893, 'train_loss': 0.39175636314049167, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05840090662240982,\n",
       " 'eval_accuracy': 0.991869918699187,\n",
       " 'eval_f1': 0.9918486169913014,\n",
       " 'eval_precision': 0.99198606271777,\n",
       " 'eval_recall': 0.991869918699187,\n",
       " 'eval_runtime': 2.5571,\n",
       " 'eval_samples_per_second': 240.503,\n",
       " 'eval_steps_per_second': 3.911,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddec690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../models/intentClassifier\n",
      "Configuration saved in ../../models/intentClassifier/config.json\n",
      "Model weights saved in ../../models/intentClassifier/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/intentClassifier/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/intentClassifier/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../../models/intentClassifier/tokenizer_config.json',\n",
       " '../../models/intentClassifier/special_tokens_map.json',\n",
       " '../../models/intentClassifier/vocab.txt',\n",
       " '../../models/intentClassifier/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"../../models/intentClassifier\")\n",
    "tokenizer.save_pretrained(\"../../models/intentClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f269af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "def predict_intent(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_class_id = outputs.logits.argmax().item()\n",
    "    return intent_labels[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53290c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scholarship/Financial Aid\n",
      "General Complaint\n",
      "Extracurricular Activities\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(predict_intent(\"How do I apply for scholarships?\"))\n",
    "print(predict_intent(\"I have a complaint about the cafeteria service?\"))\n",
    "print(predict_intent(\"Are there any upcoming student events\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "040d3b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuition/Fees Inquiry\n",
      "Tuition/Fees Inquiry\n"
     ]
    }
   ],
   "source": [
    "print(predict_intent(\"When is the tuition payment deadline?\"))\n",
    "print(predict_intent(\"Hi, I'm trying to figure out how to pay my tuition fees.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
